# Metaphor_Detection_NLP


## Code Execution Steps

1. Download the dataset from Kaggle which is available under the [link](https://www.kaggle.com/datasets/jessicali9530/celeba-dataset/data). And make sure that this folder will be present in the folder `data`.
Another file that is required for the model training is the labels / annotations file, which is present can be downloaded from [link](https://drive.google.com/file/d/1wAAZwDsHKhotxFkOYJAI57Z-LHxkiyBA/view?usp=drive_link). Place the annotations file in the `data` folder as well. So upon completing downlaod and code extraction the folder structure should look like -


## ğŸ“ Project Structure
```
metaphor-detection/
â”œâ”€â”€ data/                      # Dataset files
â”‚   â””â”€â”€ metaphor_dataset.csv   # Main dataset
â”œâ”€â”€ models/                    # Saved model weights
â”‚   â”œâ”€â”€ naive_bayes.pkl
â”‚   â”œâ”€â”€ random_forest.pkl
â”‚   â”œâ”€â”€ svm.pkl
â”‚   â”œâ”€â”€ bert_baseline/
â”‚   â””â”€â”€ melbert/
â”œâ”€â”€ notebooks/                 # Jupyter notebooks for experiments
â”‚   â”œâ”€â”€ naive_bayes.ipynb
â”‚   â”œâ”€â”€ random_forest.ipynb
â”‚   â”œâ”€â”€ svm.ipynb
â”‚   â”œâ”€â”€ decision_tree.ipynb
â”‚   â”œâ”€â”€ gradient_boosting.ipynb
â”‚   â”œâ”€â”€ baseline_bert.ipynb
â”‚   â””â”€â”€ melbert.ipynb
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py       # Data preprocessing utilities
â”‚   â”œâ”€â”€ models.py              # Model implementations
â”‚   â”œâ”€â”€ evaluation.py          # Evaluation metrics
â”‚   â””â”€â”€ utils.py               # Helper functions
â”œâ”€â”€ experiments/               # Experiment scripts
â”‚   â”œâ”€â”€ run_traditional_ml.py
â”‚   â”œâ”€â”€ run_bert_baseline.py
â”‚   â””â”€â”€ run_melbert.py
â”œâ”€â”€ results/                   # Experiment results and logs
â”‚   â”œâ”€â”€ metrics.csv
â”‚   â””â”€â”€ plots/
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # Project documentation
â””â”€â”€ .gitignore                 # Git ignore file
```

2. Now once this step is complete the required packages must be installed. The required packages for the project are available in the `requirements.txt` file. So to install these packages run the command - `pip3 install -r requirements.txt`.

3. Upon completing the installtion an additional package must be installed the `clip` package. This can only be downloaded from the OpenAI GitHub repository so run the command - `pip3 install git+https://github.com/openai/CLIP.git`

4. Now all the necessary packages will be installed and we can execute the code. First run the `CLIP.ipynb` notebook to run obtain the CLIP model weights.

5. Now run the `VAE.ipynb` notebook and which also generate weights for the LDM model.

6. And finally in the `LDM.ipynb` file the weights generated will be used to run the entire model. So in the model code we have adjusted such that the model will load the weights obtained. And upon execution of this file the complete model should be trained.

---



## ğŸ¯ Overview
This project implements multiple machine learning models to detect metaphorical language in text. The best-performing model (MelBERT) achieves 95% accuracy by combining BERT's contextualized embeddings with linguistic theories of metaphor identification.
Key Features

7 different models ranging from traditional ML to deep learning
1,870 labeled examples of metaphorical and literal text
State-of-the-art performance using MelBERT architecture
Comprehensive evaluation with accuracy, precision, recall, and F1-score

## ğŸ“Š Dataset
The dataset contains 1,870 text samples with the following structure:

metaphorID: Identifier for the metaphorical word (0-6)
label: Boolean indicating if the word is used metaphorically (True/False)
text: The paragraph containing the target word

### Class Distribution:

Positive (metaphorical): 1,432 samples
Negative (literal): 438 samples
Note: The dataset is imbalanced with ~4:1 ratio favoring metaphorical examples

### Best Worked Models:

### Baseline BERT

Architecture: Pre-trained BERT + Linear layer
Context: Full text context
Fine-tuning: Standard classification head


### MelBERT (Best Model) â­

Architecture: RoBERTa + Late interaction mechanism
Context: Â±50 words around target word
Special features: Incorporates metaphor identification theories
Performance: 95% accuracy, 96% F1-score

---
## ğŸ“ˆ Results

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| **MelBERT** | **0.95** | **0.96** | **0.97** | **0.96** |
| Baseline BERT | 0.93 | 0.97 | 0.90 | 0.93 |
| Gradient Boosting | 0.83 | 0.83 | 0.97 | 0.90 |
| Decision Tree | 0.83 | 0.83 | 0.96 | 0.89 |
| SVM | 0.82 | 0.86 | 0.90 | 0.88 |
| Naive Bayes | 0.81 | 0.87 | 0.89 | 0.88 |
| Random Forest | 0.77 | 0.77 | 1.00 | 0.87 |

### Key Findings

 **BERT-based models significantly outperform traditional ML approaches**
- MelBERT (95%) vs. best traditional model (83%)

 **Linguistic theories improve performance**
- MelBERT with theories (95%) vs. Baseline BERT (93%)

 **Context matters**
- Contextualized embeddings capture metaphorical meaning better than static representations

## ğŸ”® Future Work

- Expand dataset with more diverse examples
- Apply data augmentation techniques
- Experiment with larger language models (GPT, T5)
- Implement ensemble methods (stacking, bagging)
- Add cross-lingual metaphor detection
- Fine-tune hyperparameters using grid search

---
